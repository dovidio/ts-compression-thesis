\chapter{Approaches to Time Series compression}
\section{Storing aggregates}
Storing aggregates is probably the most common approach to deal with large time series.
The idea is to reduce the temporal resolution of data by storing aggregates of values within
some predefined time buckets. Once values have been aggregated to a lower temporal resolution,
they can be deleted, provided that the initial temporal resolution is not needed anymore.
As a practical example, let us consider an application in which we record the time delays
accumulated by trains. The following series represent the daily delay of a specific train on
a certain day in minutes.
\begin{table}[!htbp]
\centering
\begin{tabular}{l|l|l|l|l|l|l}
\textbf{Day 1} & \textbf{Day 2} & \textbf{Day 3} & \textbf{Day 4} & \textbf{Day 5} & \textbf{Day 6} & \textbf{Day 7} \\
\hline
20 & 20 & 10 & 50 & 40 & 40 & 30 \\                
\end{tabular}
\caption{Daily delay of a specific train of a certain day in minutes.}
\label{tab:daily_delay}
\end{table}

After each week passes, we want to aggregate the previous week’s daily data. With reference
to our example above, we can delete the daily values and replace them by their sum
(210 minutes). By also storing the count, we can compute the average daily during that week
(210 / 7 = 30 minutes).
This technique, often called downsampling, is so simple and effective that most time series
databases offer it out of the box. For example, OpenTSDB, an open-source time series database,
offers the possibility to query downsampled data \cite{A2019Downsample}. This is useful when
one wants to plot data keeping the density at a level that can be easily understandable by
users. Admittedly, this feature does not hold any savings in terms of storage, as it is
performed only at query time, but it is still interesting for network bound applications.
Since version 2.4 OpenTSDB offers the possibility to store the result of the downsampler
\cite{A2019Rollup}. In this way, when querying for a long time span, there is no need to wait for the
downsampled data to be computed.
InfluxDB, another popular open-source database, offers the possibility to downsample old
data using continuous queries and retention policy \cite{A2016Downsampling}. Continuous queries
are queries that are run automatically and periodically within a database. Retention policy
defines for how long InfluxDB keeps the data. By using these concepts together, one can use
a continuous query to aggregate old data into a lower temporal resolution and delete the
original data automatically by setting an appropriate retention policy.

\section{Lossless compression}
Lossless compression is a method of data compression that allows to reconstruct the original
data from the compressed data \cite{WikipediaContributors2019Data}. When applied to time series, it is used when we are
interested in retaining the original data but we want to reduce storage space. In this
section, we will present two different algorithms that have been devised to solve different
problems. The first algorithm was created at Facebook to face the huge amount of data
gathered by monitoring their systems. This algorithm has allowed them to store all data
in memory, thus improving considerably reading and writing performance.
The second algorithm we present, called Sprintz, was created to limit the energy consumption
of IoT devices. By sending compressed data, the network interface use is reduced, thus
obtaining considerable power savings.

\subsection{Gorilla}
Gorilla is an in-memory time series database used by facebook for monitoring the health of
their systems. It was initially presented in a 2015 paper, and it is now available as
open-source software under the name of Beringei \cite{beringei}. Gorilla schema is a simple tuple
composed of three values: the key of the time series stored as a string, the timestamp of the
measurement stored as an integer, and the value stored as a double. 
The compression is applied both to the timestamp and the actual double values. For the
timestamp the delta-of-delta technique is used (which we discussed in chapter 2), while for
the double values a special kind of XOR compression is applied.

\subsubsection{Timestamp compression}
Timestamp compression is achieved by storing the delta of deltas (see previous chapter)
together with a variable-length code.
The algorithm can be described as follows:
\begin{enumerate}
    \item in the block header, we store the initial timestamp $t-1$ which is aligned to a
    two-hour window. The first timestamp $t_0$ is stored as a 14-bit delta from $t-1$.
    \item For the subsequent timestamp $t_n$:
        \begin{enumerate}
            \item calculate the delta of delta $D = (t_n - t_{n-1}) - (t_{n-1} - t_{n-2})$
            \item if delta is 0, store ‘0’
            \item if delta is between [-64, 64], store ‘10’ following the 7-bit value of the
            delta
            \item if delta is between [-256, 256], store ‘110’ following with the 9-bit
            value of the delta
            \item if delta is between [-2048, 2048], store ‘1110’ following the 12-bit value
            of the delta
            \item otherwise, store ‘1111’ followed by the 32-bit value
        \end{enumerate}
\end{enumerate}
This type of compression applied to Facebook experimental data was able to achieve a 96\%
compression ratio. Figure~\ref{gorilla_timestamp} shows the result of the timestamp
compression achieved by Gorilla with Facebook’s data. It is interesting to note that
96\% of the time timestamps were compressed in one bit.

\begin{figure}[]
\begin{center}
\includegraphics[width=200pt]{gorillaTimestamp}
\caption[gorilla_timestamp]{ Distribution of timestamp compression across different ranged buckets.
Taken from a sample of 440,000 real timestamps in Gorilla \cite{Pelkonen2015Gorilla}}
\label{gorilla_timestamp}
\end{center}
\end{figure}

\subsubsection{Compressing values}
Gorilla restricts the value element in its tuple to a double floating-point type. To compress
the values, XOR compression is used together with a variable-length code.
Since values that are close together do not differ by a large amount, the sign, exponent and
few bits of the mantissa are usually the same. Gorilla exploits this by computing the XOR of
the current and previous values. We XOR’d values are then encoded using the following encoding
scheme:
\begin{enumerate}
    \item the first value is stored with no compression
    \item if the XOR with the previous value is 0, then store ‘0’
    \item when XOR is non-zero, calculate the number of leading and trailing zeros in the XOR,
    store bit ‘1’ followed by either a) or b):
        \begin{enumerate}
            \item (control bit ‘0’) If the block of meaningful bits falls within the block of
            previous meaningful bits, i.e., there are at least as many leading zeros and as many
            trailing zeros as with the previous value, use that information for the block position
            and just store the meaningful XORed value
            \item (control bit ‘1’) Store the length of the number of leading zeros in the next
            5 bits, then store the length of the meaningful XORed value in the next 6 bits.
            Finally, store the meaningful bits of the XORed value
        \end{enumerate}
\end{enumerate}
We show an example of this coding in Table~\ref{tab:gorilla}.

\begin{table}[]
\centering
\begin{tabular}{p{1.5cm}|p{3.75cm}|p{4cm}|p{3.5cm}}
\textbf{Decimal} & \textbf{Double \newline representation} & \textbf{XOR with \newline previous} & \textbf{Encoded value} \\ 
\hline
20.5  & 0x4034800000000000 & -                  & 0b010000000011010 \newline 01000000000000000 \newline 00000000000000000 \newline 000000000000000 \\
18    & 0x4032000000000000 & 0x0006800000000000 & 0b110001100000010 \newline 1000100 \\ 
21.5  & 0x4035800000000000 & 0x0007800000000000 & 0b101001110                    \\              
21    & 0x4035000000000000 & 0x0000800000000000 & 0b101000                    \\           
21.25 & 0x4035400000000000 & 0x0000C00000000000 & 0b101100                    \\            
21.25 & 0x4035400000000000 & 0x0000000000000000 & 0b0                    \\            
\end{tabular}
\caption{An example of Gorilla's value compression.}
\label{tab:gorilla}
\end{table}

Using their data set, the authors of Gorilla noted that 51\% of the time subsequent values do
not change at all so they can be stored with one bit (case 2), while 30\% of the time they
have as much leading and trailing zeroes as the previous XOR’d value (case 3a), while the
remaining 19\% are stored with the ‘11’ prefix (case 3b), with an average size of 36.9 bits
due to the 13 bits extra overhead.

\subsection{Sprintz}
Sprintz is a compression algorithm that is specifically tailored for Internet of Things (IoT)
devices \cite{Blalock2018Sprintz}. These devices have low power consumption and computational limits. Sprintz
reduces the amount of data sent over the network while still being efficient enough to be
run on low powered devices. The design requirements of Sprintz were the following:
\begin{itemize}
    \item \textbf{Small block size}. IoT devices usually do not have much memory, therefore it is not
    possible to buffer large quantities of data. Moreover using large buffers introduces high
    latency which is not favorable for many real-time applications.

    \item \textbf{High decompression speed}. Decompression speed is paramount for all those applications
    which are read-heavy. Compression speed, on the other hand, must only be fast enough to
    keep up with the ingestion rate.
    
    \item \textbf{Lossless}. Instead of assuming that some level of downsampling is appropriate for
    all data, the approach used by Sprintz is to keep the original data quality and leave to
    the developer the choice to downsample data in the preprocessing stage.
\end{itemize}
Sprintz is also targeted to multivariate time series. In a multivariate time series,
each record consists of multiple values. Each value is associated with a dimension. This
contrasts with univariate time series, whereby there is only one dimension.

\subsubsection{Sprintz components}
Sprintz can be described as a bit packing-based predictive coder \cite{Blalock2018Sprintz}.
It consists of four components:
\begin{itemize}
    \item An online forecaster, that predict the next value in the time series. Sprintz
    saves the error between the predicted value and the real value. This value is supposed
    to be 0 most of the time
    \item A bit packer, that packs the error as a payload and adds sufficient information
    to the header to invert the bit packing
    \item Run-length encoding: Sprintz stores the number of 0 blocks instead of an empty
    payload
    \item Entropy coding: Sprintz uses Huffman coding for the header and the payload
\end{itemize}

\subsubsection{Encoding}
The basic algorithm is outlined in the following pseudo-code listing:

\begin{algorithm}
\caption{Sprintz encoding algorithm \cite{Blalock2018Sprintz}}\label{sprintz_encoding}
\begin{algorithmic}[1]
\Procedure{encodeBlock}{$\{x_1, \ldots, x_b\}$, forecaster}
\State{Let buff be a temporary buffer}
\For {$i \leftarrow 1,\ldots,B$}\Comment{for each sample} \label{line:bodyStart}
    \State{$ \hat{x_i} \leftarrow $ forecaster.predict$(x_{i-1})$}
    \State{$err_i \leftarrow $ $x_i - \hat{x_i}$}
    \State{forecaster.train($x_{i-1}, x_i, err_i$)}
\EndFor
\For {$j \leftarrow 1,\ldots,D$}\Comment{for each column}
    \State{$ nbits_j \leftarrow $ $max_i$\{requiredNumBits$(err_{ij}))$\}}
    \State{$ packed_j \leftarrow $ bitPack($\{err_{1j}, \ldots, err_{Bj}\}, nbits_j$)} 
\EndFor \label{line:bodyEnd}
\State{// Run-length encode if all errors are zero}
\If{$nbits_j$ == $0$, $1 \le j \le D$}
    \Repeat  \Comment{Scan until end of run}
        \State{Read in another block and run lines \ref{line:bodyStart}-\ref{line:bodyEnd}}
    \Until {$\exists_j[nbits_j \neq 0] $}
    
    \State{Write $D$ 0s as headers into buff}
    \State{Write number of all-zero blocks as payload into buff}
    \State{Output huffmanCode(buff)}
\EndIf
\State{Write $nbits_j$, $j = 1,\ldots,D$ as headers into buff}
\State{Write $packed_j$, $j = 1,\ldots,D$ as payload into buff}
\State{Output huffmanCode(buff)}
\EndProcedure
\end{algorithmic}
\end{algorithm}

In line 3-7 the forecaster is used to predict the value, get the error of the forecaster,
and update the forecaster with the current parameters. Then in line 8-11, the maximum number
of bits needed for representing the max value is computed for each column. If all the
dimensions had a 0 error, new blocks are read until an error is found.
Run-length encoding is then used to encode the 0 errors (line 17-19). The header is written
as zeros repeated for the number of dimensions. The payload consists of the number of zero
blocks found. Header and payload are then further compressed using Huffman code. 
If non-zeroes errors were found, then the header will include a header containing the bit
width of each dimension. The values are then packed in the payload. Header and payload are
then encoded using Huffman encoding.

\subsubsection{Decoding}
The decoding is simpler than the encoding. The first step is to decode the Huffman-bitstream
into a header and a payload. Once decoded, the header is examined. If it contains all 0, it
means that the payload was run-length encoded. The number of run-length encoded blocks is
read, and each value can be reconstructed by predicting its value with the forecaster.
If the header does not contain all zeros, then after predicting the value we also need to add
the error value, which is present in the payload. The value will be the predicted value plus
the error value. The forecaster is then updated with the current parameters.

\begin{algorithm}[H]
\caption{Sprintz decoding algorithm \cite{Blalock2018Sprintz}}\label{sprintz_decoding}
\begin{algorithmic}[1]
\Procedure{decodeBlock}{bytes, $B, D$, forecaster}
\State{nbits, payload $ \leftarrow $ huffmanDecode(bytes, $B$, $D$) }
\If{$nbits_j == 0$ $ \forall j $}
    \State{numblocks $ \leftarrow$ readRunLength}
    \For {$i \leftarrow 1,\ldots,(B \times numblocks)$}
        \State{$ x_i \leftarrow $ forecaster.predict$(x_{i-1})$}
        \State{Output $x_i$}
    \EndFor
\Else
    \For {$i \leftarrow 1,\ldots,B$}
        \State{$ x_i \leftarrow $ forecaster.predict$(x_{i-1})$}
        \State{$ err_i \leftarrow $ unpackErrorVector($i$, nbits, payload)}
        \State{$ x_i \leftarrow $ $err_j + \hat{x_i}$}
        \State{Output $x_i$}
        \State{forecaster.train($x_{i-1}, x_i, err_i$)}
    \EndFor
\EndIf
\EndProcedure
\end{algorithmic}
\end{algorithm}

\subsubsection{The forecaster}
Sprintz uses a novel forecaster algorithm called FIRE (Fast Integer REgression). It is
slightly slower than delta encoding but yields a better compression ratio most of the time
\cite{Blalock2018Sprintz}. Each value is thought of as a linear combination of a fixed amount
of previous values plus some error. The details of the forecaster are outside the scope of
this thesis, but the interested reader can find more in the original paper
\cite{Blalock2018Sprintz}.

\subsubsection{Bit packing}
Explaining bit packing will be easier by using a practical example. Suppose we want to encode
the following time series:
\begin{table}[!htbp]
\centering
\begin{tabular}{l|l|l}
\textbf{X} & \textbf{Y} & \textbf{Z} \\ 
\hline
5  & 2 & 104 \\
10 & 2 & 103 \\
15 & 2 & 102 \\
20 & 2 & 101 \\
\end{tabular}
\end{table}

We first need to compute the delta. Supposing that the value immediately preceding
this series was time = -3, value 1 = 2, value 2 = 104, then the delta will look like the
following table:
\begin{table}[!htbp]
\centering
\begin{tabular}{l|l|l}
\textbf{dX} & \textbf{dY} & \textbf{dZ} \\ 
\hline
8 & 0 &  0 \\
5 & 0 & -1 \\
5 & 0 & -1 \\
5 & 0 & -1 \\
\end{tabular}
\end{table}
The next step is to apply the Zigzag encoding. Zigzag encoding is used to transform all
values in nonnegative by representing each integer by twice its absolute value, or twice
its absolute value minus one for negative integers so that positive integers are encoded
as even numbers and negative integers as odd numbers, effectively using the least significant
bit to represent the sign.
\begin{table}[!htbp]
\centering
\begin{tabular}{l|l|l}
\textbf{dX} & \textbf{dY} & \textbf{dZ} \\ 
\hline
16 & 0 &  0 \\
10 & 0 &  1 \\
10 & 0 &  1 \\
10 & 0 &  1 \\
\end{tabular}
\end{table}
The next step is to find the bit width for each column. To do so, for each value we need to
compute the current bit width minus the number of leading zeros. For each column, we take
the maximum of this value, which we will call w. For example, assuming we are 8-bit integers,
16 is represented in binary as $00010000$, so we will have $8 - 3 = 5$ bits necessary to represent
this number. 10 is represented as $00001010$ and requires only 4 bits to be represented.
Therefore we take the max of the two values, 5. The reason why Zigzag encoding was used is
that computing the number of leading zeros can be done in a single assembly instruction
(on systems that support it, such instruction is usually called \texttt{clz}). In the following table
we list the maximum number of bits required to represent each number in each column.
\begin{table}[!htbp]
\centering
\begin{tabular}{l|l|l}
\textbf{dX} & \textbf{dY} & \textbf{dZ} \\ 
\hline
5 & 0 & 1 \\
\end{tabular}
\end{table}

The final step will be to write the header and bit pack the deltas in the payload.
The header will consist of $D$ unsigned integers, one for each column, storing the bit widths.
Each integer in the header is stored in $\log_2{w}$ bits.
In the payload, values are stored contiguously.
In the following table, we use different colors for different columns. Orange will be used
for the $X$ column, green for the $Y$ column and blue for the $Z$ column.

\begin{table}[!htbp]
\centering
\begin{tabular}{l|l|l}
\textbf{Header} & \textbf{Align} & \textbf{Payload} \\ 
\hline
\textcolor{orange}{$101$} \textcolor{green}{$000$} \textcolor{blue}{$001$} & $0000000$ & \textcolor{orange}{$10000$ $01010$ $01010$ $01010$} \textcolor{blue}{$0111$} \\
\end{tabular}
\end{table}

When there are many columns, the bits for each sample are stored contiguously, adding up to
7 bits padding when the sample does not span a multiple of a byte. The reason why this is
not done when there are few columns is that having a 7-bit padding can reduce drastically
the compression ratio, and in the case of univariate data it leads to expansion.

\subsubsection{Entropy coding}
For entropy coding, a Huffman coder is used. This step is optional, but the authors have
observed increased compression ratios using it. Huffman coding is applied after bit packing
for two reasons:
\begin{enumerate}
    \item Reducing the input to the Huffman coding also reduces its latency. This is
    important as Huffman coding is slower than the rest of Sprintz
    \item A better compression ratio is achieved as compared to the case in which Huffman
    coding is applied first.
\end{enumerate}

\section{Lossy compression algorithms}
This family of algorithms uses inexact approximations to represent a data series. In
contrast with lossless compression, these algorithms do not allow to compress data and
keep the full resolution of it. In this section, we will present some algorithms that use
this approach. A feature of the algorithms that we present is that they allow for quality
guarantees to be defined, that is, users can define how much the compressed values can differ
from the original value. In the following sections, we will analyze Piecewise Constant
Approximation, Piecewise Linear Approximation, and ModelarDB, a time series database that
makes use of multiple algorithms to optimize the compression ratio based on the user-defined
quality guarantees.

\subsection{Piecewise Constant Approximation}
Piecewise constant approximation represents a time series S as a sequence of K segments
\cite{Keogh2001Locally}\cite{Lazaridis2003Capturing}:
$$PCA(S^n) = \langle(c_1, e_1), (c_2, e_2), \ldots, (c_k, e_k))\rangle$$
where $e_k$  is the end-point of a segment and $c_k$ is a constant value for time in
$[e_{k-1} + 1, e_k]$ or for time in $[1, e_1]$ for the first segment. If each segment
corresponds to many samples of the series, then high compression ratios can be achieved.
In \cite{Lazaridis2003Capturing}, a simple algorithm name Poor Man’s Compression -
Midrange (PMC-MR) was proposed to construct a PCA  representation of a series with
$O(1)$ space requirements and $O(n)$ time complexity, where $n$ is the number of samples
in a series. This algorithm does also take into consideration quality guarantees, that
is, users can decide the maximum acceptable quality loss the algorithm will produce.
Algorithm~\ref{PMC_MR} lists the procedure taken from \cite{Lazaridis2003Capturing}.

\begin{algorithm}
\caption{Poor Man's Compression Midrange \cite{Lazaridis2003Capturing}}\label{PMC_MR}
\begin{algorithmic}[1]
\Procedure{pcr}{series, tolerance}
\State{PCA(series) $ \leftarrow $ $\langle \rangle$ }
\State{n $ \leftarrow $ $1$ }
\State{m $ \leftarrow $ series[n] }
\State{M $ \leftarrow $ series[n] }
\While{series.hasMoreSamples()}
    \If{max\{M, series[n]\} - min\{m, series[n]\} $>$ 2 x tolerance}
        \State{\textbf{append}($\frac{M + m}{2}, n-1$) \textbf{to} PCA(series)}
        \State{m $ \leftarrow $ series[n] }
        \State{M $ \leftarrow $ series[n] }
    \Else
        \State{m $ \leftarrow $ min\{m, series[n]\} }
        \State{M $ \leftarrow $ max\{M, series[n]\}}
    \EndIf
    \State{n $ \leftarrow $ n + 1 }
\EndWhile
\State{\textbf{append}($\frac{M + m}{2}, n-1$) \textbf{to} PCA(series)}
\EndProcedure
\end{algorithmic}
\end{algorithm}

The procedure uses $n$ to represent the endpoint of the segment, and represents the
minimum and maximum values found in the segment not yet pushed to the PCA with $m$ and
$M$. In each cycle, we evaluate whether the range we are considering is no more than
twice the quality boundary. If that is the case, we update $m$ and $N$. Once the current
range exceeds this quality boundary, we push $\frac{m + M}{2}$  up until $n - 1$ and we
reset $m$ and $M$ to the current value (which was not included in the current segment).
As a practical example, suppose we have the following series:
$$S = 22, 24, 31, 32, 33, 37$$
If our quality guarantee is 3 (that is, the absolute value of the difference between the
compressed value and the original value should never exceed 3), the series will be
compressed to the following PCA:
$$PCA = \langle(23, 2), (34, 6) \rangle$$
In Figure~\ref{pca} we have a visual representation of how the series has changed.

\begin{figure}[!htbp]
\begin{center}
\includegraphics[width=400pt]{pca}
\caption[pca]{A time series before and after applying the PMC-MR algorithm.}
\label{pca}
\end{center}
\end{figure}